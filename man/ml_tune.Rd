% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ml_tune.R
\name{ml_tune}
\alias{ml_tune}
\title{Function to tune ML algos for multiple time series forecasting}
\usage{
ml_tune(
  parsnip_recipe,
  modeltime_recipe,
  vfold,
  grid_size,
  cv_repeats,
  parallel_type = "everything",
  return = c("modellist", "modeltable", "both"),
  models = c("xgboost", "rf", "cubist", "svm_rbf", "svm_poly", "glmnet", "knn", "mars",
    "prophet_boost", "lightgbm", "catboost"),
  learn_rate = NULL,
  min_n = NULL,
  tree_depth = NULL,
  loss_reduction = NULL
)
}
\arguments{
\item{parsnip_recipe}{Recipe for parsnip models}

\item{modeltime_recipe}{Recipe for modeltime models}

\item{vfold}{Number of folds used in K-fold cross validation}

\item{grid_size}{The size of the hyperparameter grid used for tuning the parameters}

\item{cv_repeats}{How many CV repeats to use}

\item{return}{What do you want to return. List of workflows, modeltime table or both?}

\item{models}{Choose which models to use. Choose any combination of xgboost, rf, cubist, svm_rbf, svm_poly, glmnet, knn, mars or prophet_boost}

\item{learn_rate}{Upper and lower bound of learning rate to try out during tuning. NULL equals default values from the dials package}

\item{min_n}{Upper and lower bound of min_n to try out during tuning. NULL equals default values from the dials package}

\item{tree_depth}{Upper and lower bound of tree_depth to try out during tuning. NULL equals default values from the dials package}

\item{loss_reduction}{Upper and lower bound of loss_reduction to try out during tuning. NULL equals default values from the dials package}

\item{parallel_over}{A single string containing either "resamples" or "everything" describing how to use parallel processing. See ?control_grid}
}
\description{
ml_tune() will tune up to nine different ML algorithms used in forecasting
}
